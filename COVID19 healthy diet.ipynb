{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cabeçalho - TODO (escrever bonitinho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumo do trabalho - TODO (escrever bonitinho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando bibliotecas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO - dando xabu na matplotlib e xgboost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%lsmagic\n",
    "\n",
    "import time\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definindo funções úteis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graficos_boxplot(X, colunas=None, grupo=None, restricoes=None):\n",
    "    \n",
    "    if grupo == None:\n",
    "        print ('nenhum grupo informado')\n",
    "        return None\n",
    "    \n",
    "    if colunas == None: colunas = X.columns.tolist()\n",
    "    \n",
    "    dado = X[colunas + [grupo]].copy()\n",
    "    \n",
    "    for col in colunas:\n",
    "        plt.figure(figsize=(16, 4))\n",
    "        ax = sns.boxplot(x=grupo, y=col, data=dado)\n",
    "        plt.setp(ax.artists, alpha=.5, linewidth=2, edgecolor=\"k\")\n",
    "        plt.xticks(rotation=45)\n",
    "    return\n",
    "\n",
    "def detectar_outliers(X, colunas=None, metodo='one class svm', contamination=0.1):\n",
    "    \n",
    "    if colunas == None: colunas = X.columns.tolist()\n",
    "    \n",
    "    x_local = X[colunas].copy()\n",
    "    \n",
    "    col_num = x_local.select_dtypes(include = ['float64', 'int64']).columns.tolist()\n",
    "    col_cat = x_local.select_dtypes(include = ['object']).columns.tolist()\n",
    "    \n",
    "    for col in col_cat:\n",
    "        enc = LabelEncoder()\n",
    "        x_local[col] = enc.fit_transform(x_local[col].astype(str))\n",
    "        \n",
    "    for col in col_num:\n",
    "        x_local[col].fillna(0, inplace=True)\n",
    "        \n",
    "    x_local['is outlier'] = 0\n",
    "    \n",
    "    dado = x_local[colunas]\n",
    "    \n",
    "    isolation_forest = IsolationForest(n_estimators=100, behaviour='new', contamination=contamination)\n",
    "    robust_covariance = EllipticEnvelope(contamination=contamination)\n",
    "    OneClassSVM = svm.OneClassSVM(kernel='poly', degree=2, gamma=1000, tol=0.1, nu=contamination)\n",
    "    local_out_factor = LocalOutlierFactor(n_neighbors=35, contamination=contamination)\n",
    "    \n",
    "    if metodo == 'one class svm': out = OneClassSVM\n",
    "    if metodo == 'isolation forest': out = isolation_forest\n",
    "    if metodo == 'robust covariance': out = robust_covariance\n",
    "    if metodo == 'local outlier fraction': out = local_out_factor\n",
    "    \n",
    "    if metodo == 'local outlier fraction':\n",
    "        dado['is outlier'] = out.fit_predict(dado)\n",
    "    else:\n",
    "        out.fit(dado)\n",
    "        dado['is outlier'] = out.predict(dado)\n",
    "    \n",
    "    mask = dado[dado['is outlier'] == -1].index.tolist()\n",
    "    x_local.loc[mask, 'is outlier'] = 1\n",
    "    \n",
    "    return x_local['is outlier']\n",
    "\n",
    "def preencher_buracos(X, coluna, valor = None, coluna_grupo = None):\n",
    "    if valor != None: \n",
    "        X[coluna].fillna(valor, inplace=True)\n",
    "    elif coluna_grupo == coluna:\n",
    "        X[coluna].fillna(X[coluna].mean())\n",
    "    else:\n",
    "        mask = X[coluna].isnull()\n",
    "        replace_dict = X[coluna, coluna_grupo].groupby(coluna_grupo).mean().to_dict()[coluna]\n",
    "        X.loc[mask, coluna] = X[coluna_grupo][mask].replace(replace_dict)\n",
    "    return X\n",
    "\n",
    "def normalizar_numericas(X, colunas_numericas = None):\n",
    "    if colunas_numericas == None: colunas_numericas = X.columns.tolist()\n",
    "    scaler = RobustScaler()\n",
    "    X[colunas_numericas] = scaler.fit_transform(X[colunas_numericas])\n",
    "    return X, scaler\n",
    "\n",
    "def codificar_categoricas(X, colunas_categoricas = None):\n",
    "    if colunas_categoricas == None: colunas_categoricas = X.columns.tolist()\n",
    "    colunas_numericas = [x for x in X.columns.tolist() if x not in colunas_categoricas]\n",
    "    enc = OneHotEncoder(handle_unknown='ignore')\n",
    "    X_cat = pd.DataFrame(enc.fit_transform(X[colunas_categoricas]).toarray())\n",
    "    X.reset_index(inplace=True, drop=True)\n",
    "    res = pd.concat([X_cat, X[colunas_numericas]], axis=1)\n",
    "    return res, enc\n",
    "\n",
    "def fitar_modelo(modelo, X, y, X_test = None, y_test = None):\n",
    "    modelo.fit(X, y)\n",
    "    train_score = r2_score(y, modelo.predict(X))\n",
    "    test_score = r2_score(y_test, modelo.predict(X_test))\n",
    "    if X_test == None:\n",
    "        return modelo, train_score\n",
    "    else:\n",
    "        return modelo, train_score, test_score\n",
    "\n",
    "def importancia_das_features(X, y, colunas_numericas = None, colunas_categoricas = None, nome_variavel = ''): \n",
    "    \n",
    "    x_local = X.copy()\n",
    "    y_local = y.copy()\n",
    "    \n",
    "    if colunas_numericas != None: x_local, _ = normalizar_numericas(x_local, colunas_numericas)\n",
    "    if colunas_categoricas != None:\n",
    "        for col in colunas_categoricas:\n",
    "            enc = LabelEncoder()\n",
    "            x_local[col] = enc.fit_transform(x_local[col].astype(str))\n",
    "    \n",
    "    features_names = x_local.columns\n",
    "    \n",
    "    rf_importance = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
    "    rf_importance.fit(x_local, y_local)\n",
    "    \n",
    "    feature_importances = list(rf_importance.feature_importances_)\n",
    "    list_tuples = zip(features_names, feature_importances)\n",
    "    \n",
    "    plt.figure(123)\n",
    "    plt.figure(figsize=(5, 10))\n",
    "    features_names_sort, feature_importances_sort = zip(*sorted(list_tuples, key=lambda x:x[1], reverse=False)[:50])\n",
    "    plt.barh(range(len(feature_importances_sort)), feature_importances_sort)\n",
    "    plt.yticks(np.arange(len(features_names_sort)), features_names_sort)\n",
    "    plt.title('Feature Importances - ' + nome_variavel)\n",
    "    plt.show()\n",
    "    \n",
    "    return features_names_sort, feature_importances_sort\n",
    "\n",
    "def calcular_correlação(X, colunas_numericas=None, colunas_categoricas=None):\n",
    "    \n",
    "    x_local = X.copy()\n",
    "    colunas = []\n",
    "    \n",
    "    if colunas_numericas != None:\n",
    "        x_local, _ = normalizar_numericas(x_local, colunas_numericas)\n",
    "        colunas = colunas + colunas_numericas\n",
    "    if colunas_categoricas != None:\n",
    "        colunas = colunas + colunas_categoricas\n",
    "        for col in colunas_categoricas:\n",
    "            enc = LabelEncoder()\n",
    "            x_local[col] = enc.fit_transform(x_local[col].astype(str))\n",
    "    \n",
    "    correlacao = {}\n",
    "    for col in colunas:\n",
    "        val = x_local.drop(col, axis=1).apply(lambda x: x.corr(x_local[col]))\n",
    "        correlacao[col] = val.sort_values(ascending=False)\n",
    "    \n",
    "    df = pd.DataFrame(correlacao)\n",
    "            \n",
    "    return df\n",
    "\n",
    "def selecionar_features(X, y, y_name=None, colunas_numericas=None, colunas_categoricas=None, limite_corr=0.8, limite_importancia=0.8, quantidade=999):\n",
    "    \n",
    "    x_local = X.copy()\n",
    "    y_local = y.copy()\n",
    "    \n",
    "    if colunas_numericas != None and colunas_categoricas != None:\n",
    "        corr = calcular_correlação(x_local, colunas_numericas=colunas_numericas, colunas_categoricas=colunas_categoricas)\n",
    "        features_names_sort, feature_importances_sort = importancia_das_features(x_local, y_local, colunas_numericas=colunas_numericas, colunas_categoricas=colunas_categoricas, nome_variavel = y_name)\n",
    "    elif colunas_numericas != None and colunas_categoricas == None:\n",
    "        corr = calcular_correlação(x_local, colunas_numericas=colunas_numericas)\n",
    "        features_names_sort, feature_importances_sort = importancia_das_features(x_local, y_local, colunas_numericas=colunas_numericas, nome_variavel = y_name)\n",
    "    elif colunas_numericas == None and colunas_categoricas != None:\n",
    "        corr = calcular_correlação(x_local, colunas_categoricas=colunas_categoricas)\n",
    "        features_names_sort, feature_importances_sort = importancia_das_features(x_local, y_local, colunas_categoricas=colunas_categoricas, nome_variavel = y_name)\n",
    "    else:\n",
    "        print (\"algo errado ai meu camarada\")\n",
    "        return None\n",
    "    \n",
    "    features = corr.columns.tolist()\n",
    "    features_slct = []\n",
    "    count = 0\n",
    "    for f, v in zip(reversed(features_names_sort), reversed(feature_importances_sort)):\n",
    "        print (f, v)\n",
    "        if count > quantidade: break\n",
    "        add = True\n",
    "        if float(v) > limite_importancia: add = False\n",
    "        for f_f in features_slct:\n",
    "            if v < 0.001 or abs(corr.loc[f, f_f]) > limite_corr: add = False\n",
    "        if add: \n",
    "            features_slct.append(f)\n",
    "            count += 1\n",
    "    \n",
    "    x_features_slct = x_local[features_slct]\n",
    "    return x_features_slct, features_slct\n",
    "\n",
    "def busca_aleatoria(X, y, modelo, parametros, metrica, n_folds = 10, n_iter = 200):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    print (\"\\n \\n buscando melhores parâmetros...\")\n",
    "    \n",
    "    otm_model = RandomizedSearchCV(modelo, param_distributions=parametros, n_iter=n_iter, scoring=metrica, cv=n_folds, n_jobs=1)\n",
    "    otm_model.fit(X, y)\n",
    "    \n",
    "    melhores_parametros = otm_model.best_params_\n",
    "    melhor_modelo = otm_model.best_estimator_\n",
    "    \n",
    "    resultados = pd.DataFrame(otm_model.cv_results_)\n",
    "    resultados.sort_values(by='rank_test_score', inplace=True)\n",
    "    resultados.sort_values(by='rank_test_score', inplace=True)\n",
    "    print (\"melhores parâmetros encontrados\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    otm_time = (end_time - start_time) / 60\n",
    "    \n",
    "    best_score = otm_model.best_score_\n",
    "    \n",
    "    return melhor_modelo, melhores_parametros, best_score, resultados, otm_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando o dado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fat_df = pd.read_csv('./data/Fat_Supply_Quantity_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_df = pd.read_csv('./data/Protein_Supply_Quantity_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kcal_df = pd.read_csv('./data/Food_Supply_kcal_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kcal_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantity_kg_df = pd.read_csv('./data/Food_Supply_Quantity_kg_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantity_kg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_df = pd.read_csv('./data/Supply_Food_Data_Descriptions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Juntando datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [fat_df, protein_df, kcal_df, quantity_kg_df, description_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in dfs:\n",
    "    print (df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = {}\n",
    "i = 0\n",
    "for df in dfs[:-1]:\n",
    "    i += 1\n",
    "    cols['df'+str(i)] = list(df.columns)\n",
    "df_cols = pd.DataFrame(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_cols = ['Country', 'Obesity', 'Undernourished', 'Confirmed', 'Deaths', 'Recovered', 'Active', 'Population', 'Unit (all except Population)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fat_df[shared_cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(fat_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols = ['Country'] + [i for i in cols if i not in shared_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fat_df = fat_df[cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_df = protein_df[cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kcal_df = kcal_df[cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantity_kg_df = quantity_kg_df[cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fat_cols_map = {i:i+'_fat' for i in cols if i != 'Country'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_cols_map = {i:i+'_protein' for i in cols if i != 'Country'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kcal_cols_map = {i:i+'_kcal' for i in cols if i != 'Country'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_cols_map = {i:i+'_quantity' for i in cols if i != 'Country'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fat_df.rename(fat_cols_map, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_df.rename(protein_cols_map, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kcal_df.rename(kcal_cols_map, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantity_kg_df.rename(quant_cols_map, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.merge(fat_df, how='left', on='Country', copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.merge(protein_df, how='left', on='Country', copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.merge(kcal_df, how='left', on='Country', copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.merge(quantity_kg_df, how='left', on='Country', copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisando o dado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num = dataset.select_dtypes(include = ['float64', 'int64'])\n",
    "data_cat = dataset.select_dtypes(include = ['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, val in zip(dataset.isnull().sum().index.to_list(), list(dataset.isnull().sum().values)):\n",
    "    if val > 0:\n",
    "        print (col, val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[dataset['Confirmed'].isnull() != True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, val in zip(dataset.isnull().sum().index.to_list(), list(dataset.isnull().sum().values)):\n",
    "    if val > 0:\n",
    "        print (col, val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Undernourished'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.loc[dataset['Undernourished'] == '<2.5', 'Undernourished'] = '2.5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Undernourished'] = dataset['Undernourished'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Obesity'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Obesity'] = dataset['Obesity'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Obesity'].fillna(dataset['Obesity'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Undernourished'].fillna(dataset['Undernourished'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isnull().sum().unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Golden features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['Confirmed', 'Deaths', 'Recovered', 'Active']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_num = dataset.select_dtypes(include = ['float64', 'int64'])\n",
    "for t in targets:\n",
    "    data_corr = data_num.corr()[t]\n",
    "    golden_features_list = data_corr[abs(data_corr) > 0.2].sort_values(ascending=False)\n",
    "    print (t, golden_features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: fazer uns gráficos das correlações (golden features)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remover outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_outlier = data_num.columns.to_list()\n",
    "\n",
    "dataset['is outlier'] = detectar_outliers(dataset, colunas=colunas_outlier, metodo='local outlier fraction', \n",
    "                                          contamination=0.01)\n",
    "print (str(list(dataset['is outlier']).count(1)) + ' outliers detectados')\n",
    "dataset = dataset[dataset['is outlier']==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separando features das targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['Obesity', 'Undernourished', 'Confirmed', 'Deaths', 'Recovered', 'Active']\n",
    "y_dict = {}\n",
    "for t in targets:\n",
    "    y_dict[t] = dataset[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = dataset.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [f for f in cols if f not in targets]\n",
    "\n",
    "x_features = dataset[features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x_features['Unit (all except Population)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_features.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num = x_features.select_dtypes(include = ['float64', 'int64'])\n",
    "data_cat = x_features.select_dtypes(include = ['object'])\n",
    "colunas_a_normalizar = data_num.columns.tolist()\n",
    "colunas_a_codificar = data_cat.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_importances_dict = {}\n",
    "features_names_dict = {}\n",
    "for t in targets:\n",
    "    features_names_sort, feature_importances_sort = importancia_das_features(x_features, y_dict[t], colunas_numericas=colunas_a_normalizar, colunas_categoricas=colunas_a_codificar, nome_variavel = t)\n",
    "    features_importances_dict[t] = feature_importances_sort\n",
    "    features_names_dict[t] = features_names_sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: colorir gráfico de acordo com sinal (negativo ou positivo) da correlação**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
